# Statistics
## Mean of dataset
$$ \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i $$
$\bar{x}_{n+1}=\bar{x}_{n-1}+\frac{1}{n}(x_i-\bar{x}_{n-1})$

$D = \{x_1, x_2, \dots, x_n\}, E= \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2 $
```python
import numpy as np

def reshape(x):
    """return x_reshaped as a flattened vector of the multi-dimensional array x"""
    x_reshaped = x.reshape(-1)  # æˆ–è€…ç”¨ x.flatten()
    return x_reshaped

# ç¯„ä¾‹æ¸¬è©¦
img = np.arange(28*28).reshape(28, 28)  # å»ºç«‹ä¸€å€‹ 28x28 çš„æ¸¬è©¦å½±åƒ
print(reshape(img).shape)  # (784,)
```
### Effect on the mean
D: dataset
- $D+2$ then $E+2$
- $D*2$ then $E*2$

## Variance of dataset
$$ \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2 $$
å…¶ä¸­ $\mu$ æ˜¯æ•¸æ“šé›†çš„å¹³å‡å€¼ã€‚

å°æ–¼æ¨£æœ¬æ–¹å·®ï¼Œæˆ‘å€‘é€šå¸¸ä½¿ç”¨ $n-1$ ä½œç‚ºåˆ†æ¯ï¼Œä»¥æä¾›ç„¡åä¼°è¨ˆï¼š
$$ s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2 $$
å…¶ä¸­ $\bar{x}$ æ˜¯æ¨£æœ¬å¹³å‡å€¼ã€‚

$\sigma_{n}^2 = \frac{n-1}{n}\sigma_{n-1}^2 + \frac{1}{n}(x_i - \mu_{n-1})(x_i - \mu_{n})$


$Var[D] = E[(D - \mu)(D-\mu)^T]$

### Effect on the Variance
D: dataset 
- $D*2$ then Variance $is 4*Var(D)$
- $D+2$ then Variance is $Var(D)$

where $A$ is a matrix.

$Var[AD] = E[(AD-A\mu)(AD-\mu)^T] = E[A(D-\mu)A^T(D-\mu)^T]$

$ = E[A(D-\mu)(D-\mu)^TA^T] = AVar[D]A^T$

## Covariance of dataset
$$ Cov(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) $$
å…¶ä¸­ $\bar{x}$ å’Œ $\bar{y}$ åˆ†åˆ¥æ˜¯ $X$ å’Œ $Y$ çš„å¹³å‡å€¼ã€‚

å°æ–¼æ¨£æœ¬å”æ–¹å·®ï¼Œæˆ‘å€‘ä½¿ç”¨ $n-1$ ä½œç‚ºåˆ†æ¯ï¼š
$$ s_{xy} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) $$

- $Cov(X, Y) = E[(X - E[X])(Y - E[Y])]$
- $Cov(X, Y) = E[XY] - E[X]E[Y]$
- $Cov(X, X) = Var(X)$
- $Cov(aX + b, cY + d) = ac \ Cov(X, Y)$
- å¦‚æœ $X$ å’Œ $Y$ ç¨ç«‹ï¼Œå‰‡ $Cov(X, Y) = 0$ (åä¹‹ä¸ä¸€å®šæˆç«‹)

$\begin{bmatrix} Cov(x,x) & Cov(x,y) \\ Cov(y,x) & Cov(y,y) \end{bmatrix} = \begin{bmatrix} Var(x) & Cov(x,y) \\ Cov(y,x) & Var(y) \end{bmatrix}$

### Effect on the Covariance
D: dataset
- $D+2$ then Cov(D)
- $D*2$ then Cov(D)

## Correlation
$$ \rho_{XY} = \frac{Cov(X, Y)}{\sigma_X \sigma_Y} $$
å…¶ä¸­ $\sigma_X$ å’Œ $\sigma_Y$ åˆ†åˆ¥æ˜¯ $X$ å’Œ $Y$ çš„æ¨™æº–å·®ã€‚

- ç›¸é—œä¿‚æ•¸ $\rho_{XY}$ çš„å€¼ä»‹æ–¼ -1 å’Œ 1 ä¹‹é–“ã€‚
- $\rho_{XY} = 1$ è¡¨ç¤ºå®Œå…¨æ­£ç·šæ€§ç›¸é—œã€‚
- $\rho_{XY} = -1$ è¡¨ç¤ºå®Œå…¨è² ç·šæ€§ç›¸é—œã€‚
- $\rho_{XY} = 0$ è¡¨ç¤ºæ²’æœ‰ç·šæ€§ç›¸é—œï¼ˆä½†å¯èƒ½å­˜åœ¨éç·šæ€§ç›¸é—œï¼‰ã€‚

# Inner Product
## Dot product
$x^T y = \sum_{i=1}^{n} x_i y_i $ where $x , y \in R^N$

$x = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, y = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$

$||x|| = \sqrt{x^Tx} = \sqrt{\sum_{i=1}^{n} x_i^2} = \sqrt{1^2 + 2^2} = \sqrt{5}$

$||y|| = \sqrt{\sum_{i=1}^{n} y_i^2} = \sqrt{2^2 + 1^2} = \sqrt{5}$

$d(x,y) = ||x-y|| = \sqrt{(x-y)^T(x-y)} = \sqrt{\sum_{i=1}^{n} (x_i-y_i)^2} = \sqrt{(1-2)^2 + (2-1)^2} = \sqrt{1+1} = \sqrt{2}$

$cos(\theta) = \frac{x^T y}{||x|| ||y||} = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \sqrt{\sum_{i=1}^{n} y_i^2}} = \frac{1 \times 2 + 2 \times 1}{\sqrt{5} \times \sqrt{5}} = \frac{4}{5} = 0.8$

$\theta = \arccos(cos(\theta)) = \arccos(0.8) = 0.65 rad$

```python
import numpy as np

def length(x):
  """Compute the length of a vector"""
  length_x = np.linalg.norm(x) # <--- compute the length of a vector x here.
  
  return length_x
  
print(length(np.array([1,0])))
```

### Definition
é»ç©ï¼ˆDot Productï¼‰ï¼Œä¹Ÿç¨±ç‚ºç´”é‡ç©ï¼ˆScalar Productï¼‰ï¼Œæ˜¯å…©å€‹å‘é‡çš„ä¹˜ç©ï¼Œå…¶çµæœæ˜¯ä¸€å€‹ç´”é‡ã€‚

å°æ–¼å…©å€‹ $n$ ç¶­å‘é‡ $\mathbf{a} = [a_1, a_2, \dots, a_n]$ å’Œ $\mathbf{b} = [b_1, b_2, \dots, b_n]$ï¼Œå®ƒå€‘çš„é»ç©å®šç¾©ç‚ºå°æ‡‰åˆ†é‡çš„ä¹˜ç©ä¹‹å’Œï¼š
$$ \mathbf{a} \cdot \mathbf{b} = \mathbf{a}^T \mathbf{b} $$
æˆ–è€…å¯«æˆï¼š$$ \mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i $$é€™ä¹Ÿå¯ä»¥å¯«æˆï¼š
$$ \mathbf{a} \cdot \mathbf{b} = \begin{bmatrix} a_1 & a_2 & \dots & a_n \end{bmatrix} \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} $$

- Symmetric äº¤æ›å¾‹ï¼š$\mathbf{a} \cdot \mathbf{b} = \mathbf{b} \cdot \mathbf{a}$
- Positive definite : $\mathbf{a} \cdot \mathbf{a} \ge 0$ï¼Œä¸” $\mathbf{a} \cdot \mathbf{a} = 0$ è‹¥ä¸”å”¯è‹¥ $\mathbf{a} = \mathbf{0}$
- Bilinear å°æ–¼å…©å€‹åƒæ•¸éƒ½æ˜¯ç·šæ€§çš„ã€‚

#### Example
Letâ€™s check each property of

$$\beta(\mathbf{x}, \mathbf{y}) = \mathbf{x}^T\begin{bmatrix} 2 & -1 \\ -1 & 1 \end{bmatrix} \mathbf{y}$$
1. Bilinearity

    This expression is of the form $x^T A y$ with a constant matrix $A$. Such a function is **bilinear**:

    * Linear in $\mathbf{x}$ when $\mathbf{y}$ is fixed.
    * Linear in $\mathbf{y}$ when $\mathbf{x}$ is fixed.
  
2. Symmetry

    Symmetric means $\beta(x,y) = \beta(y,x)$ for all $x, y$.
    The matrix $$A = \begin{bmatrix} 2 & -1 \\ -1 & 1 \end{bmatrix}$$
    is symmetric ($A = A^T$), so $\beta(x,y) = x^T A y = y^T A x = \beta(y,x).$

3. Positive Definiteness

    An inner product must satisfy $\beta(x,x) > 0$ for all $x \neq 0$.

    Check $ \beta(x,x) = x^T A x$:

    $$x^T A x = x_1^2(2) + x_1x_2(-1) + x_2x_1(-1) + x_2^2(1) = 2x_1^2 - 2x_1x_2 + x_2^2.$$

    Test $x = (1,1)^T$:

    $$= 2(1)^2 - 2(1)(1) + (1)^2 = 2 - 2 + 1 = 1 > 0.$$

    Test $x = (2,4)^T$:

    $$= 2(4) - 2(8) + 16 = 8 - 16 + 16 = 8 > 0.$$

    Test $x = (1,2)^T$:

    $$= 2(1) - 2(2) + 4 = 2 - 4 + 4 = 2 > 0.$$

    Now check for any zero or negative values: the determinant of $A$ is $ (2)(1) - (-1)^2 = 2 - 1 = 1 > 0$ and the leading principal minors are positive ($2 > 0$, det $= 1 > 0$), so $A$ is positive definite.

4. Inner Product

    Since $\beta$ is bilinear, symmetric, and positive definite, it satisfies all inner product axioms.

---

$$ \mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \dots + a_n b_n $$
åœ¨å¹¾ä½•å­¸ä¸­ï¼Œé»ç©çš„å®šç¾©ç‚ºï¼š
$$ \mathbf{a} \cdot \mathbf{b} = ||\mathbf{a}|| \ ||\mathbf{b}|| \cos(\theta) $$
å…¶ä¸­ $||\mathbf{a}||$ å’Œ $||\mathbf{b}||$ åˆ†åˆ¥æ˜¯å‘é‡ $\mathbf{a}$ å’Œ $\mathbf{b}$ çš„é•·åº¦ï¼ˆæˆ–ç¯„æ•¸ï¼‰ï¼Œ$\theta$ æ˜¯å…©å€‹å‘é‡ä¹‹é–“çš„å¤¾è§’ã€‚

é»ç©çš„æ€§è³ªï¼š
- **äº¤æ›å¾‹**ï¼š$\mathbf{a} \cdot \mathbf{b} = \mathbf{b} \cdot \mathbf{a}$
- **åˆ†é…å¾‹**ï¼š$\mathbf{a} \cdot (\mathbf{b} + \mathbf{c}) = \mathbf{a} \cdot \mathbf{b} + \mathbf{a} \cdot \mathbf{c}$
- **èˆ‡ç´”é‡ä¹˜æ³•çš„çµåˆå¾‹**ï¼š$(c\mathbf{a}) \cdot \mathbf{b} = \mathbf{a} \cdot (c\mathbf{b}) = c(\mathbf{a} \cdot \mathbf{b})$
- **éè² æ€§**ï¼š$\mathbf{a} \cdot \mathbf{a} \ge 0$ï¼Œä¸” $\mathbf{a} \cdot \mathbf{a} = 0$ è‹¥ä¸”å”¯è‹¥ $\mathbf{a} = \mathbf{0}$
- **é•·åº¦å¹³æ–¹**ï¼š$\mathbf{a} \cdot \mathbf{a} = ||\mathbf{a}||^2$

```python
import numpy as np

def dot(a, b):
  """Compute dot product between a and b.
  Args:
    a, b: (2,) ndarray as R^2 vectors
  
  Returns:
    a number which is the dot product between a, b
  """
  dot_product = np.dot(a, b)
  return dot_product

# Test your code before you submit.
a = np.array([1,0])
b = np.array([0,1])
print(dot(a,b))
```

### Length of vectors
$||x|| = \sqrt{<x,x>}$ where x>=0

$<x,y> = x^Ty$ => $||x|| = \sqrt{2}$

$<x,y> = x^T \begin{bmatrix} 1 & \frac{-1}{2} \\ \frac{-1}{2} & 1 \end{bmatrix} y$


---

## ğŸ“Š çŸ©é™£ä¸‰å…„å¼Ÿæ¯”è¼ƒè¡¨

| åç¨±                                     | å®šç¾©                 | è¨ˆç®—æ–¹å¼                                | å…¬å¼ (ä»¥ 2Ã—2 ç‚ºä¾‹)                                                                                               | ç”¨é€”         | å®¹æ˜“ææ··çš„åœ°æ–¹                  |
| -------------------------------------- | ------------------ | ----------------------------------- | ----------------------------------------------------------------------------------------------------------- | ---------- | ------------------------ |
| **è½‰ç½®** (Transpose, $A^T$)              | æŠŠè¡Œå’Œåˆ—äº’æ›             | $(A^T)_{ij} = A_{ji}$               | $$\begin{bmatrix} a & b \\ c & d \end{bmatrix}^T = \begin{bmatrix} a & c \\ b & d \end{bmatrix}$$             | å°ç¨±æ€§æª¢æŸ¥ã€å…§ç©è¨ˆç®— | **ä¸æ¶‰åŠè¡Œåˆ—å¼**ï¼Œåªæ˜¯ä½ç½®äº’æ›        |
| **ä¼´éš¨çŸ©é™£** (Adjugate, $\mathrm{adj}(A)$) | æŠŠçŸ©é™£çš„**ä»£æ•¸é¤˜å­å¼çŸ©é™£**å†è½‰ç½® | å…ˆç®—æ¯å€‹å…ƒç´ çš„ä»£æ•¸é¤˜å­å¼ï¼Œå†æ•´å€‹çŸ©é™£è½‰ç½®                | $$\mathrm{adj}\begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$$ | è¨ˆç®—é€†çŸ©é™£çš„ä¸­é–“æ­¥é©Ÿ | å¾ˆå¤šäººä»¥ç‚ºé€™æ˜¯è½‰ç½®ï¼Œ**å…¶å¯¦æ˜¯å®Œå…¨ä¸åŒçš„æ±è¥¿** |
| **é€†çŸ©é™£** (Inverse, $A^{-1}$)            | æ»¿è¶³ $AA^{-1} = I$   | $\frac{1}{\det(A)} \mathrm{adj}(A)$ | $$A^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$$                                   | è§£ç·šæ€§æ–¹ç¨‹çµ„     | è¦æ±‚ $\det(A) \neq 0$ æ‰èƒ½å­˜åœ¨ |

---