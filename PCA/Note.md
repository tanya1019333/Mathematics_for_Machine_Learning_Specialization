# Statistics
## Mean of dataset
$$ \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i $$
é€™æ˜¯ä¸€å€‹å¸¸ç”¨çš„éè¿´æ›´æ–°å…¬å¼ï¼Œç”¨æ–¼åœ¨åŠ å…¥æ–°è³‡æ–™é» $x_n$ æ™‚æ›´æ–°å¹³å‡å€¼ï¼š
$$ \bar{x}_{n}=\bar{x}_{n-1}+\frac{1}{n}(x_n-\bar{x}_{n-1}) $$

$D = \{x_1, x_2, \dots, x_n\}, \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2 $
```python
import numpy as np

def reshape(x):
    """return x_reshaped as a flattened vector of the multi-dimensional array x"""
    # æˆ–è€…ç”¨ x.flatten()ã€‚
    # æ³¨æ„ï¼šreshape(-1) åœ¨å¯èƒ½çš„æƒ…æ³ä¸‹æœƒå›å‚³ä¸€å€‹è¦–åœ– (view)ï¼Œä¸ä½”ç”¨æ–°è¨˜æ†¶é«”ï¼Œ
    # è€Œ flatten() ç¸½æ˜¯å›å‚³ä¸€å€‹å‰¯æœ¬ (copy)ã€‚
    x_reshaped = x.reshape(-1)
    return x_reshaped

# ç¯„ä¾‹æ¸¬è©¦
img = np.arange(28*28).reshape(28, 28)  # å»ºç«‹ä¸€å€‹ 28x28 çš„æ¸¬è©¦å½±åƒ
print(reshape(img).shape)  # (784,)
```
### Effect on the mean
D: dataset
- $D+2$ then $E+2$
- $D*2$ then $E*2$

## Variance of dataset
$$ \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2 $$
å…¶ä¸­ $\mu$ æ˜¯æ•¸æ“šé›†çš„å¹³å‡å€¼ã€‚

å°æ–¼æ¨£æœ¬æ–¹å·®ï¼Œæˆ‘å€‘é€šå¸¸ä½¿ç”¨ $n-1$ ä½œç‚ºåˆ†æ¯ï¼Œä»¥æä¾›ç„¡åä¼°è¨ˆï¼š
$$ s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2 $$
å…¶ä¸­ $\bar{x}$ æ˜¯æ¨£æœ¬å¹³å‡å€¼ã€‚

$\sigma_{n}^2 = \frac{n-1}{n}\sigma_{n-1}^2 + \frac{1}{n}(x_i - \mu_{n-1})(x_i - \mu_{n})$


$Var[D] = E[(D - \mu)(D-\mu)^T]$

### Effect on the Variance
D: dataset 
- $D*2$ then Variance $is 4*Var(D)$
- $D+2$ then Variance is $Var(D)$

where $A$ is a matrix.

$Var[AD] = E[(AD-A\mu)(AD-\mu)^T] = E[A(D-\mu)A^T(D-\mu)^T]$

$ = E[A(D-\mu)(D-\mu)^TA^T] = AVar[D]A^T$

## Covariance of dataset
$$ Cov(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) $$
å…¶ä¸­ $\bar{x}$ å’Œ $\bar{y}$ åˆ†åˆ¥æ˜¯ $X$ å’Œ $Y$ çš„å¹³å‡å€¼ã€‚

å°æ–¼æ¨£æœ¬å”æ–¹å·®ï¼Œæˆ‘å€‘ä½¿ç”¨ $n-1$ ä½œç‚ºåˆ†æ¯ï¼š
$$ s_{xy} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) $$

- $Cov(X, Y) = E[(X - E[X])(Y - E[Y])]$
- $Cov(X, Y) = E[XY] - E[X]E[Y]$
- $Cov(X, X) = Var(X)$
- $Cov(aX + b, cY + d) = ac \ Cov(X, Y)$
- å¦‚æœ $X$ å’Œ $Y$ ç¨ç«‹ï¼Œå‰‡ $Cov(X, Y) = 0$ (åä¹‹ä¸ä¸€å®šæˆç«‹)

$\begin{bmatrix} Cov(x,x) & Cov(x,y) \\ Cov(y,x) & Cov(y,y) \end{bmatrix} = \begin{bmatrix} Var(x) & Cov(x,y) \\ Cov(y,x) & Var(y) \end{bmatrix}$

### Effect on the Covariance
D: dataset
- $D+2$ then Cov(D)
- $D*2$ then Cov(D)

## Correlation
$$ \rho_{XY} = \frac{Cov(X, Y)}{\sigma_X \sigma_Y} $$
å…¶ä¸­ $\sigma_X$ å’Œ $\sigma_Y$ åˆ†åˆ¥æ˜¯ $X$ å’Œ $Y$ çš„æ¨™æº–å·®ã€‚

- ç›¸é—œä¿‚æ•¸ $\rho_{XY}$ çš„å€¼ä»‹æ–¼ -1 å’Œ 1 ä¹‹é–“ã€‚
- $\rho_{XY} = 1$ è¡¨ç¤ºå®Œå…¨æ­£ç·šæ€§ç›¸é—œã€‚
- $\rho_{XY} = -1$ è¡¨ç¤ºå®Œå…¨è² ç·šæ€§ç›¸é—œã€‚
- $\rho_{XY} = 0$ è¡¨ç¤ºæ²’æœ‰ç·šæ€§ç›¸é—œï¼ˆä½†å¯èƒ½å­˜åœ¨éç·šæ€§ç›¸é—œï¼‰ã€‚

# Inner Product
## Dot product
$x^T y = \sum_{i=1}^{n} x_i y_i $ where $x , y \in R^N$

$x = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, y = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$

$||x|| = \sqrt{x^Tx} = \sqrt{\sum_{i=1}^{n} x_i^2} = \sqrt{1^2 + 2^2} = \sqrt{5}$

$||y|| = \sqrt{\sum_{i=1}^{n} y_i^2} = \sqrt{2^2 + 1^2} = \sqrt{5}$

$d(x,y) = ||x-y|| = \sqrt{(x-y)^T(x-y)} = \sqrt{\sum_{i=1}^{n} (x_i-y_i)^2} = \sqrt{(1-2)^2 + (2-1)^2} = \sqrt{1+1} = \sqrt{2}$

$cos(\theta) = \frac{x^T y}{||x|| ||y||} = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \sqrt{\sum_{i=1}^{n} y_i^2}} = \frac{1 \times 2 + 2 \times 1}{\sqrt{5} \times \sqrt{5}} = \frac{4}{5} = 0.8$

$\theta = \arccos(cos(\theta)) = \arccos(0.8) = 0.65 rad$

```python
import numpy as np

def length(x):
  """Compute the length of a vector"""
  length_x = np.linalg.norm(x) # <--- compute the length of a vector x here.
  
  return length_x
  
print(length(np.array([1,0])))
```

### Definition
é»ç©ï¼ˆDot Productï¼‰ï¼Œä¹Ÿç¨±ç‚ºç´”é‡ç©ï¼ˆScalar Productï¼‰ï¼Œæ˜¯å…©å€‹å‘é‡çš„ä¹˜ç©ï¼Œå…¶çµæœæ˜¯ä¸€å€‹ç´”é‡ã€‚

å°æ–¼å…©å€‹ $n$ ç¶­å‘é‡ $\mathbf{a} = [a_1, a_2, \dots, a_n]$ å’Œ $\mathbf{b} = [b_1, b_2, \dots, b_n]$ï¼Œå®ƒå€‘çš„é»ç©å®šç¾©ç‚ºå°æ‡‰åˆ†é‡çš„ä¹˜ç©ä¹‹å’Œï¼š
$$ \mathbf{a} \cdot \mathbf{b} = \mathbf{a}^T \mathbf{b} $$
æˆ–è€…å¯«æˆï¼š$$ \mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i $$é€™ä¹Ÿå¯ä»¥å¯«æˆï¼š
$$ \mathbf{a} \cdot \mathbf{b} = \begin{bmatrix} a_1 & a_2 & \dots & a_n \end{bmatrix} \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} $$

- Symmetric äº¤æ›å¾‹ï¼š$\mathbf{a} \cdot \mathbf{b} = \mathbf{b} \cdot \mathbf{a}$
- Positive definite : $\mathbf{a} \cdot \mathbf{a} \ge 0$ï¼Œä¸” $\mathbf{a} \cdot \mathbf{a} = 0$ è‹¥ä¸”å”¯è‹¥ $\mathbf{a} = \mathbf{0}$
- Bilinear å°æ–¼å…©å€‹åƒæ•¸éƒ½æ˜¯ç·šæ€§çš„ã€‚

#### Example
Letâ€™s check each property of

$$\beta(\mathbf{x}, \mathbf{y}) = \mathbf{x}^T\begin{bmatrix} 2 & -1 \\ -1 & 1 \end{bmatrix} \mathbf{y}$$
1. Bilinearity

    This expression is of the form $x^T A y$ with a constant matrix $A$. Such a function is **bilinear**:

    * Linear in $\mathbf{x}$ when $\mathbf{y}$ is fixed.
    * Linear in $\mathbf{y}$ when $\mathbf{x}$ is fixed.
  
2. Symmetry

    Symmetric means $\beta(x,y) = \beta(y,x)$ for all $x, y$.
    The matrix $$A = \begin{bmatrix} 2 & -1 \\ -1 & 1 \end{bmatrix}$$
    is symmetric ($A = A^T$), so $\beta(x,y) = x^T A y = y^T A x = \beta(y,x).$

3. Positive Definiteness

    An inner product must satisfy $\beta(x,x) > 0$ for all $x \neq 0$.

    Check $ \beta(x,x) = x^T A x$:

    $$x^T A x = x_1^2(2) + x_1x_2(-1) + x_2x_1(-1) + x_2^2(1) = 2x_1^2 - 2x_1x_2 + x_2^2.$$

    Test $x = (1,1)^T$:

    $$= 2(1)^2 - 2(1)(1) + (1)^2 = 2 - 2 + 1 = 1 > 0.$$

    Test $x = (2,4)^T$:

    $$= 2(4) - 2(8) + 16 = 8 - 16 + 16 = 8 > 0.$$

    Test $x = (1,2)^T$:

    $$= 2(1) - 2(2) + 4 = 2 - 4 + 4 = 2 > 0.$$

    Now check for any zero or negative values: the determinant of $A$ is $ (2)(1) - (-1)^2 = 2 - 1 = 1 > 0$ and the leading principal minors are positive ($2 > 0$, det $= 1 > 0$), so $A$ is positive definite.

4. Inner Product

    Since $\beta$ is bilinear, symmetric, and positive definite, it satisfies all inner product axioms.

---

$$ \mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \dots + a_n b_n $$
åœ¨å¹¾ä½•å­¸ä¸­ï¼Œé»ç©çš„å®šç¾©ç‚ºï¼š
$$ \mathbf{a} \cdot \mathbf{b} = ||\mathbf{a}|| \ ||\mathbf{b}|| \cos(\theta) $$
å…¶ä¸­ $||\mathbf{a}||$ å’Œ $||\mathbf{b}||$ åˆ†åˆ¥æ˜¯å‘é‡ $\mathbf{a}$ å’Œ $\mathbf{b}$ çš„é•·åº¦ï¼ˆæˆ–ç¯„æ•¸ï¼‰ï¼Œ$\theta$ æ˜¯å…©å€‹å‘é‡ä¹‹é–“çš„å¤¾è§’ã€‚

é»ç©çš„æ€§è³ªï¼š
- **äº¤æ›å¾‹**ï¼š$\mathbf{a} \cdot \mathbf{b} = \mathbf{b} \cdot \mathbf{a}$
- **åˆ†é…å¾‹**ï¼š$\mathbf{a} \cdot (\mathbf{b} + \mathbf{c}) = \mathbf{a} \cdot \mathbf{b} + \mathbf{a} \cdot \mathbf{c}$
- **èˆ‡ç´”é‡ä¹˜æ³•çš„çµåˆå¾‹**ï¼š$(c\mathbf{a}) \cdot \mathbf{b} = \mathbf{a} \cdot (c\mathbf{b}) = c(\mathbf{a} \cdot \mathbf{b})$
- **éè² æ€§**ï¼š$\mathbf{a} \cdot \mathbf{a} \ge 0$ï¼Œä¸” $\mathbf{a} \cdot \mathbf{a} = 0$ è‹¥ä¸”å”¯è‹¥ $\mathbf{a} = \mathbf{0}$
- **é•·åº¦å¹³æ–¹**ï¼š$\mathbf{a} \cdot \mathbf{a} = ||\mathbf{a}||^2$

```python
import numpy as np

def dot(a, b):
  """Compute dot product between a and b.
  Args:
    a, b: (2,) ndarray as R^2 vectors
  
  Returns:
    a number which is the dot product between a, b
  """
  dot_product = np.dot(a, b)
  return dot_product

# Test your code before you submit.
a = np.array([1,0])
b = np.array([0,1])
print(dot(a,b))
```

### Length of vectors
$||x|| = \sqrt{<x,x>}$ where x>=0 $$<x,y> = x^Ty => ||x|| = \sqrt{2}$$
$$<x,y> = x^T \begin{bmatrix} 1 & \frac{-1}{2} \\ \frac{-1}{2} & 1 \end{bmatrix} y = x_1y_1 - \frac{1}{2}(x_2y_1+x_1y_2)+ x_2y_2$$
$$||x|| = \sqrt{x_1^2 - \frac{1}{2}(x_1x_2+x_2x_1)+ x_2^2} = \sqrt{x_1^2 - (x_1x_2)+ x_2^2}$$
$$||x||^2 = 1+1-1 = 1$$
$$||\lambda x|| = \lambda ||x||$$
ä¸‰è§’ä¸ç­‰å¼(Triangle inequality)ï¼š
$$||x+y|| \le ||x|| + ||y||$$
Cauchy-Schwart inequalityï¼š
$$||<x,y>|| \le ||x|| + ||y||$$

### Distances between vectors
$$d(x,y) = ||x-y||$$
### **å·²çŸ¥æ¢ä»¶**
$$\mathbf{x} = \begin{bmatrix} 1 \\ -1 \\ 3 \end{bmatrix}$$

å…§ç©å®šç¾©ï¼š
$$\langle a, b \rangle = a^T M b$$

å…¶ä¸­
$$M = \begin{bmatrix} 2 & 1 & 0 \\ 1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}$$

å‘é‡çš„é•·åº¦ï¼ˆnormï¼‰å®šç¾©ç‚ºï¼š

$\|\mathbf{x}\| = \sqrt{\langle x, x \rangle}$

### **æ­¥é©Ÿ 1ï¼šè¨ˆç®— $Mx$**

$$
M \cdot x = \begin{bmatrix} 2 & 1 & 0 \\ 1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix} \begin{bmatrix} 1 \\ -1 \\ 3 \end{bmatrix} = \begin{bmatrix} 2(1) + 1(-1) + 0(3) \\ 1(1) + 2(-1) + (-1)(3) \\ 0(1) + (-1)(-1) + 2(3) \end{bmatrix} = \begin{bmatrix} 1 \\ -4 \\ 7 \end{bmatrix}$$

### **æ­¥é©Ÿ 2ï¼šè¨ˆç®— $x^T (Mx)$**

$$
x^T (Mx) =
\begin{bmatrix} 1 & -1 & 3 \end{bmatrix}
\begin{bmatrix} 1 \\ -4 \\ 7 \end{bmatrix}
= 1(1) + (-1)(-4) + 3(7)
= 1 + 4 + 21
= 26
$$

### **æ­¥é©Ÿ 3ï¼šå–å¹³æ–¹æ ¹å¾—åˆ°é•·åº¦**

$$\|\mathbf{x}\| = \sqrt{\langle x, x \rangle} = \sqrt{26}$$
$$\boxed{\sqrt{26}}$$

## Angles & Orthogonality
$$ \cos(\theta) = \frac{\langle x, y \rangle}{||x|| \ ||y||} $$
å…¶ä¸­ $\langle x, y \rangle$ æ˜¯å‘é‡ $x$ å’Œ $y$ çš„å…§ç©ï¼Œ$||x||$ å’Œ $||y||$ åˆ†åˆ¥æ˜¯å®ƒå€‘çš„é•·åº¦ã€‚

**æ­£äº¤æ€§ (Orthogonality)**

å¦‚æœå…©å€‹å‘é‡çš„å…§ç©ç‚ºé›¶ï¼Œå‰‡ç¨±å®ƒå€‘æ˜¯æ­£äº¤çš„ã€‚
$$ \langle x, y \rangle = 0 $$
é€™è¡¨ç¤ºå…©å€‹å‘é‡ä¹‹é–“çš„å¤¾è§’ç‚º $90^\circ$ï¼ˆæˆ– $\frac{\pi}{2}$ å¼§åº¦ï¼‰ï¼Œå³å®ƒå€‘äº’ç›¸å‚ç›´ã€‚

**ç¯„ä¾‹**

å‡è¨­æˆ‘å€‘æœ‰å…©å€‹å‘é‡ $x = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ å’Œ $y = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ã€‚

1.  **è¨ˆç®—å…§ç©**ï¼š
    $$ \langle x, y \rangle = x^T y = \begin{bmatrix} 1 & 0 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} = 1 \cdot 0 + 0 \cdot 1 = 0 $$

2.  **è¨ˆç®—é•·åº¦**ï¼š
    $$ ||x|| = \sqrt{1^2 + 0^2} = 1 $$
    $$ ||y|| = \sqrt{0^2 + 1^2} = 1 $$

3.  **è¨ˆç®—å¤¾è§’é¤˜å¼¦**ï¼š
    $$ \cos(\theta) = \frac{0}{1 \cdot 1} = 0 $$

4.  **è¨ˆç®—å¤¾è§’**ï¼š
    $$ \theta = \arccos(0) = 90^\circ \text{ æˆ– } \frac{\pi}{2} \text{ å¼§åº¦} $$

ç”±æ–¼å…§ç©ç‚ºé›¶ï¼Œå‘é‡ $x$ å’Œ $y$ æ˜¯æ­£äº¤çš„ã€‚

```python
# the matrix A defines the inner product
A = np.array([[1, -1/2],[-1/2,5]])
x = np.array([0,-1])
y = np.array([1,1])

def find_angle(A, x, y):
    """Compute the angle"""
    inner_prod = x.T @ A @ y
    # Fill in the expression for norm_x and norm_y below
    norm_x = np.sqrt(x.T @ A @ x)
    norm_y = np.sqrt(y.T @ A @ y)
    alpha = inner_prod/(norm_x*norm_y)
    angle = np.arccos(alpha)
    return np.round(angle,2) 

find_angle(A, x, y)
```
```python
# Fill in the following arrays and use `find_angle` to aim your calculation.
A = np.array([[1,0,0],[0,2,-1],[0,-1,3]])
x = np.array([1,1,1])
y = np.array([2,-1,0])

find_angle(A, x, y)
```

# Orthogonal Projection
## projection onto 1D
1. $\pi_U(x) \in U => \exist \lambda \in R : \pi_U(x) = \lambda b$ (as $\pi_U(x) \in U$)
2. $\langle b, \pi_U(x)-x \rangle = 0$ (orthogonality)
<=> $$\langle b, \pi_U(x) \rangle - \langle b,x \rangle  =0$$ 
<=> $$\langle b,\lambda b \rangle - \lambda \langle b,x \rangle$$
<=> $$\lambda ||b||^2 - \langle b,x \rangle $$
<=> $$\lambda = \frac{\langle b,x \rangle}{||b||^2}$$

    => $\pi_U(x) = \lambda b = \frac{\langle b,x \rangle b}{||b||^2}$

### Example
$\pi_U(x) = \frac{x^Tbb}{||b||^2}$
where $x = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$ and $b = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$

$\pi_U(x) = \frac{4}{5} \begin{bmatrix} 2 \\ 1 \end{bmatrix}$

If a projection matrix = $\frac{bb^T}{||b||^2}$ then 
- It is a square matrix, i.e., the number of columns equals the number of rows.
- It is symmetric.($bb^T = (bb^T)^T$)

### Reconstruction error (Euclidean distance)
$x=\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix},$ projection of $x = \frac{1}{9}\begin{bmatrix} 5  \\ 10 \\ 10 \end{bmatrix}$ then x - projection of x = $\begin{bmatrix} 1-\frac{5}{9} \\ 1-\frac{10}{9} \\ 1-\frac{10}{9} \end{bmatrix} = \begin{bmatrix} \frac{4}{9} \\ -\frac{1}{9} \\ -\frac{1}{9} \end{bmatrix}$
$$\sqrt{(\frac{4}{9})^2 + (-\frac{1}{9})^2 + (-\frac{1}{9})^2} = \sqrt{\frac{16}{81}} = \frac{\sqrt{2}}{3} = 0.47... $$

## Projections onto higher-dimensional subspaces
$$u = [b_1, b_2], \pi_U(x) = \lambda_1b_1+\lambda_2b_2$$
$$\langle x-\pi_U(x), b_1 \rangle = 0, \langle x-\pi_U(x), b_2 \rangle = 0$$
---

$\lambda = \begin{bmatrix} \lambda_1 \\ \lambda_2 \\ ... \\ \lambda_M \end{bmatrix}_{M\times1}, B = \begin{bmatrix} b_1 & b_2 & ... & b_M \end{bmatrix}_{D\times M}$
1. $\pi_U(x) = \sum_{i=1}^M \lambda_i b_i$
2. $\langle \pi_U(x) - x, b_i \rangle = 0, i = 1,...,M$

Therefore
$$\pi_U(x) = B\lambda$$

$$\langle \pi_U(x)-x, b_i \rangle = \langle B\lambda - x, b_i \rangle = 0 ?$$

<=> $ \langle B \lambda, b_i \rangle - \langle B \lambda, x \rangle = 0, i = 1, ..., M$

<=> $ \lambda^T B^T bi \lambda - \lambda^T B^T x = 0$

<=> $\lambda^TB^TB - x^TB = 0$

<=> $\lambda^T = x^TB(B^TB)^{-1}$

<=> $\lambda = (B^TB)^{-1}B^Tx$

=> $\pi_U(x) = B(B^TB)^{-1}B^Tx$ â†’ **Projection Matrix**

## projection onto 2D
### Example
where $x = \begin{bmatrix} 2 \\ 1 \\ 1 \end{bmatrix}, b_1 = \begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix}, b_2 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$
$$u = [b_1, b_2], \pi_U(x) = \lambda_1b_1+\lambda_2b_2$$
$$\langle x-\pi_U(x), b_1 \rangle = 0, \langle x-\pi_U(x), b_2 \rangle = 0$$
$$B = \begin{bmatrix} b_1 & b_2 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 2 & 1 \\ 0 & 0 \end{bmatrix}$$
$$\lambda = (B^TB)^{-1}B^Tx$$

$B^T = \begin{bmatrix} 1 & 2 & 0 \\ 1 & 1 & 0 \end{bmatrix}, B^Tx = \begin{bmatrix} 4 \\ 3 \end{bmatrix}, B^TB = \begin{bmatrix} 5 & 3 \\ 3 & 2 \end{bmatrix}, (B^TB)^{-1} = \begin{bmatrix} 2 & -3 \\ -3 & 5 \end{bmatrix}$
$$\lambda = \begin{bmatrix} -1 \\ 3 \end{bmatrix}, \pi_U(x) = B\lambda = -1b_1+3b_2 =  \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}$$


---

## ğŸ“Š çŸ©é™£ä¸‰å…„å¼Ÿæ¯”è¼ƒè¡¨

| åç¨±                                     | å®šç¾©                 | è¨ˆç®—æ–¹å¼                                | å…¬å¼ (ä»¥ 2Ã—2 ç‚ºä¾‹)                                                                                               | ç”¨é€”         | å®¹æ˜“ææ··çš„åœ°æ–¹                  |
| -------------------------------------- | ------------------ | ----------------------------------- | ----------------------------------------------------------------------------------------------------------- | ---------- | ------------------------ |
| **è½‰ç½®** (Transpose, $A^T$)              | æŠŠè¡Œå’Œåˆ—äº’æ›             | $(A^T)_{ij} = A_{ji}$               | $$\begin{bmatrix} a & b \\ c & d \end{bmatrix}^T = \begin{bmatrix} a & c \\ b & d \end{bmatrix}$$             | å°ç¨±æ€§æª¢æŸ¥ã€å…§ç©è¨ˆç®— | **ä¸æ¶‰åŠè¡Œåˆ—å¼**ï¼Œåªæ˜¯ä½ç½®äº’æ›        |
| **ä¼´éš¨çŸ©é™£** (Adjugate, $\mathrm{adj}(A)$) | æŠŠçŸ©é™£çš„**ä»£æ•¸é¤˜å­å¼çŸ©é™£**å†è½‰ç½® | å…ˆç®—æ¯å€‹å…ƒç´ çš„ä»£æ•¸é¤˜å­å¼ï¼Œå†æ•´å€‹çŸ©é™£è½‰ç½®                | $$\mathrm{adj}\begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$$ | è¨ˆç®—é€†çŸ©é™£çš„ä¸­é–“æ­¥é©Ÿ | å¾ˆå¤šäººä»¥ç‚ºé€™æ˜¯è½‰ç½®ï¼Œ**å…¶å¯¦æ˜¯å®Œå…¨ä¸åŒçš„æ±è¥¿** |
| **é€†çŸ©é™£** (Inverse, $A^{-1}$)            | æ»¿è¶³ $AA^{-1} = I$   | $\frac{1}{\det(A)} \mathrm{adj}(A)$ | $$A^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$$                                   | è§£ç·šæ€§æ–¹ç¨‹çµ„     | è¦æ±‚ $\det(A) \neq 0$ æ‰èƒ½å­˜åœ¨ |

---